dsds
Excellent idea. The example you provided showcases several modern and robust practices from the pydantic-ai ecosystem. By applying these patterns, we can significantly improve your Confluence analysis script, making it more resilient, observable, and efficient.

Here's a refactored version of your script that incorporates the key improvements from the example.

Summary of Improvements:

Asynchronous Networking with httpx: Replaced the synchronous requests library with httpx.AsyncClient for true non-blocking I/O. This is crucial for performance in an asyncio application.

Observability with logfire: Integrated logfire to automatically instrument the agent and the httpx client. This will give you detailed traces of the LLM calls, tool usage, and API requests, which is invaluable for debugging.

Resilience with ModelRetry: Tools now raise a ModelRetry exception on failure (e.g., if a Confluence page isn't found). This allows the agent to intelligently retry the operation, potentially with corrected parameters, instead of failing immediately. The retries=3 parameter is also added to the agent.

Handling Synchronous Libraries: The atlassian-python-api is synchronous. To use it in an async context without blocking, its methods are now run in a separate thread pool using asyncio.to_thread. This is the correct and modern way to integrate blocking code.

Cleaner Dependencies: The AsyncClient is now part of the AnalysisDependencies dataclass and its lifecycle is managed cleanly in the main function using an async with block.

Enhanced Debugging: Uses devtools.debug() for a much richer and more readable output of the final result object compared to printing a raw dictionary.

Modern Python Practices: Includes from __future__ import annotations for better type hinting.

Prerequisites

Before running, you'll need to install the new dependencies:

Generated bash
pip install pydantic-ai "openai>=1.12" atlassian-python-api httpx devtools "logfire[pydantic-ai]"


You can also create a requirements.txt file with this content.

The Improved Script
Generated python
from __future__ import annotations as _annotations

import os
import json
import asyncio
from dataclasses import dataclass
from typing import Optional, Dict, List, Any

import httpx
import logfire
from devtools import debug
from atlassian import Confluence
from pydantic import BaseModel, Field
from pydantic_ai import Agent, RunContext, ModelRetry
from pydantic_ai.models.openai import OpenAIModel
from pydantic_ai.providers.openai import OpenAIProvider

# Configure logfire for observability (works even without a token)
logfire.configure(send_to_logfire='if-token-present')
logfire.instrument_pydantic_ai()

# ============================================================================
# ASYNC-COMPATIBLE CONFLUENCE TOOLKIT
# ============================================================================

class ConfluenceToolkit:
    """
    An async-compatible wrapper for the synchronous atlassian-python-api library.
    It uses asyncio.to_thread to avoid blocking the event loop.
    """
    def __init__(self, url: str, username: str, api_token: str):
        self.confluence = Confluence(
            url=url,
            username=username,
            password=api_token,
            cloud=True
        )
    
    async def get_confluence_spaces(self) -> List[Dict]:
        """Get all Confluence spaces"""
        spaces = await asyncio.to_thread(self.confluence.get_all_spaces, start=0, limit=50)
        return spaces.get('results', [])
    
    async def search_confluence_using_cql(self, cql: str) -> List[Dict]:
        """Search Confluence using CQL"""
        results = await asyncio.to_thread(self.confluence.cql, cql)
        return results.get('results', [])
    
    async def get_confluence_page(self, page_id: str) -> Dict:
        """Get a specific Confluence page"""
        page = await asyncio.to_thread(
            self.confluence.get_page_by_id,
            page_id, 
            expand='body.storage,space,version'
        )
        return page
    
    async def get_pages_in_confluence_space(self, space_key: str) -> List[Dict]:
        """Get all pages in a Confluence space"""
        pages = await asyncio.to_thread(
            self.confluence.get_all_pages_from_space,
            space=space_key, 
            start=0, 
            limit=25
        )
        return pages

# ============================================================================
# DEPENDENCIES AND OUTPUT MODELS
# ============================================================================

@dataclass
class AnalysisDependencies:
    """Dependencies passed to agent tools and functions"""
    client: httpx.AsyncClient
    confluence_toolkit: ConfluenceToolkit
    pr_number: str
    repo_name: str
    pr_title: str
    pr_body: str
    file_analysis: Dict

class ConfluenceAction(BaseModel):
    """A single Confluence documentation action"""
    action: str = Field(description="Type of action: update_page, create_page, or review_page")
    page_id: Optional[str] = Field(description="Confluence page ID if updating existing page")
    space_key: str = Field(description="Confluence space key")
    page_title: str = Field(description="Title of the page to update or create")
    reason: str = Field(description="Why this documentation needs updating")
    priority: str = Field(description="Priority level: high, medium, or low")
    specific_changes: str = Field(description="Specific changes or additions needed")
    existing_content_summary: Optional[str] = Field(description="Brief summary of current page content if applicable")

class ConfluenceAnalysisResult(BaseModel):
    """Final structured output of the Confluence analysis"""
    confluence_actions: List[ConfluenceAction] = Field(description="List of documentation actions needed")
    summary: str = Field(description="Brief summary of documentation impact and rationale")
    total_actions: int = Field(description="Total number of actions identified")
    estimated_effort: str = Field(description="Estimated effort: Low, Medium, or High")
    spaces_affected: List[str] = Field(description="List of Confluence spaces that will be affected")

# ============================================================================
# PYDANTICAI AGENT WITH TOOLS
# ============================================================================

provider = OpenAIProvider(api_key=os.getenv('OPENAI_API_KEY'))
model = OpenAIModel('gpt-4o', provider=provider)

confluence_agent = Agent[AnalysisDependencies, ConfluenceAnalysisResult](
    model,
    deps_type=AnalysisDependencies,
    output_type=ConfluenceAnalysisResult,
    retries=3,  # Add retries for resilience
    system_prompt="""You are an elite documentation strategist... [Your detailed prompt here]"""
)

@confluence_agent.tool
async def get_confluence_spaces(ctx: RunContext[AnalysisDependencies]) -> str:
    """Get all available Confluence spaces with their keys and names."""
    try:
        with logfire.span("tool: get_confluence_spaces"):
            spaces = await ctx.deps.confluence_toolkit.get_confluence_spaces()
            simplified = [{"key": s.get("key"), "name": s.get("name")} for s in spaces]
            return json.dumps(simplified, indent=2)
    except Exception as e:
        raise ModelRetry(f"Error getting Confluence spaces: {e}")

@confluence_agent.tool
async def search_confluence_using_cql(ctx: RunContext[AnalysisDependencies], cql: str) -> str:
    """Search Confluence using a CQL query string."""
    try:
        with logfire.span("tool: search_confluence_using_cql", cql=cql):
            results = await ctx.deps.confluence_toolkit.search_confluence_using_cql(cql)
            if not results:
                return "No pages found for this CQL query."
            simplified = [{"id": r.get("id"), "title": r.get("title"), "space": r.get("space", {}).get("key")} for r in results]
            return json.dumps(simplified, indent=2)
    except Exception as e:
        raise ModelRetry(f"Error searching Confluence with CQL '{cql}': {e}")

@confluence_agent.tool
async def get_confluence_page(ctx: RunContext[AnalysisDependencies], page_id: str) -> str:
    """Get detailed content of a specific Confluence page by its ID."""
    try:
        with logfire.span("tool: get_confluence_page", page_id=page_id):
            page = await ctx.deps.confluence_toolkit.get_confluence_page(page_id)
            if not page or "error" in page:
                raise ModelRetry(f"Page with ID '{page_id}' not found or error retrieving it.")
            simplified = {"id": page.get("id"), "title": page.get("title"), "content_preview": page.get("body", {}).get("storage", {}).get("value", "")[:1000]}
            return json.dumps(simplified, indent=2)
    except Exception as e:
        raise ModelRetry(f"Error getting Confluence page '{page_id}': {e}")

# ============================================================================
# MAIN ANALYZER CLASS
# ============================================================================

class PRConfluenceAnalyzer:
    def __init__(self, client: httpx.AsyncClient):
        self.client = client
        self.confluence_toolkit = ConfluenceToolkit(
            url=os.getenv('CONFLUENCE_URL'),
            username=os.getenv('CONFLUENCE_USERNAME'),
            api_token=os.getenv('CONFLUENCE_API_TOKEN')
        )
        self.pr_number = os.getenv('PR_NUMBER')
        self.repo_name = os.getenv('REPO_NAME')
        self.pr_title = os.getenv('PR_TITLE', '')
        self.pr_body = os.getenv('PR_BODY', '')
    
    async def get_pr_changes(self) -> List[Dict]:
        """Get PR file changes from GitHub API using httpx."""
        url = f"https://api.github.com/repos/{self.repo_name}/pulls/{self.pr_number}/files"
        headers = {'Authorization': f'token {os.getenv("GITHUB_TOKEN")}', 'Accept': 'application/vnd.github.v3+json'}
        try:
            response = await self.client.get(url, headers=headers)
            response.raise_for_status()
            return response.json()
        except httpx.HTTPStatusError as e:
            print(f"‚ùå GitHub API error: {e.response.status_code} - {e.response.text}")
            return []
        except Exception as e:
            print(f"‚ùå Error fetching PR changes: {e}")
            return []
            
    def analyze_file_changes(self, files: List[Dict]) -> Dict:
        """Analyze the file changes to understand impact (remains synchronous)."""
        # ... [This function's logic is CPU-bound and doesn't need to be async] ...
        analysis = {'total_files': len(files), 'files_by_type': {}, 'significant_changes': []}
        for f in files:
            ext = f['filename'].split('.')[-1] if '.' in f['filename'] else 'no-ext'
            analysis['files_by_type'][ext] = analysis['files_by_type'].get(ext, 0) + 1
            if f.get('additions', 0) > 20 or f.get('deletions', 0) > 10:
                analysis['significant_changes'].append(f['filename'])
        return analysis
    
    async def run_analysis(self) -> ConfluenceAnalysisResult | None:
        """Run the main Confluence analysis using PydanticAI."""
        print("üîç Getting PR file changes...")
        files = await self.get_pr_changes()
        if not files:
            print("‚ùå Could not retrieve PR file changes. Aborting.")
            return None
        
        file_analysis = self.analyze_file_changes(files)
        
        deps = AnalysisDependencies(
            client=self.client,
            confluence_toolkit=self.confluence_toolkit,
            pr_number=self.pr_number,
            repo_name=self.repo_name,
            pr_title=self.pr_title,
            pr_body=self.pr_body,
            file_analysis=file_analysis
        )
        
        analysis_prompt = f"""
        üöÄ **CONFLUENCE DOCUMENTATION IMPACT ANALYSIS**
        **PR CONTEXT:** PR #{deps.pr_number} in {deps.repo_name}: "{deps.pr_title}"
        **CODE CHANGES:** {json.dumps(deps.file_analysis, indent=2)}
        **INSTRUCTIONS:** Analyze the PR context and code changes to identify necessary Confluence documentation updates. Use the available tools to explore Confluence, find relevant pages, and then formulate a detailed plan.
        """
        
        try:
            print("ü§ñ Starting PydanticAI analysis...")
            result = await confluence_agent.run(analysis_prompt, deps=deps)
            return result.data
        except Exception as e:
            logfire.error("‚ùå Error during PydanticAI analysis: {e}", e=e)
            print(f"‚ùå Error during PydanticAI analysis: {e}")
            return None

# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def main():
    print("üöÄ Starting PR Confluence Analysis with PydanticAI")
    print("=" * 60)
    
    required_vars = ['OPENAI_API_KEY', 'CONFLUENCE_URL', 'CONFLUENCE_USERNAME', 'CONFLUENCE_API_TOKEN', 'GITHUB_TOKEN', 'PR_NUMBER', 'REPO_NAME']
    if any(not os.getenv(var) for var in required_vars):
        missing = [var for var in required_vars if not os.getenv(var)]
        print(f"‚ùå Missing required environment variables: {', '.join(missing)}")
        return

    async with httpx.AsyncClient() as client:
        logfire.instrument_httpx(client)
        
        analyzer = PRConfluenceAnalyzer(client)
        print(f"üìã Analyzing PR #{analyzer.pr_number}: {analyzer.pr_title}")
        
        result = await analyzer.run_analysis()
        
        print("\n" + "="*60)
        print("üìä CONFLUENCE ANALYSIS RESULTS")
        print("="*60)
        
        if result:
            debug(result)  # Use devtools for rich output
            # Save structured results to a file
            with open('confluence_actions.json', 'w') as f:
                json.dump(result.model_dump(), f, indent=2)
            print("\n‚úÖ Analysis successful. Structured results saved to confluence_actions.json")
        else:
            print("‚ùå Analysis failed.")
        
        print("\nüéâ Analysis complete!")

if __name__ == "__main__":
    asyncio.run(main())
IGNORE_WHEN_COPYING_START
content_copy
download
Use code with caution.
Python
IGNORE_WHEN_COPYING_END
